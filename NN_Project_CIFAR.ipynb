{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fed21/Neural-Network-Project/blob/main/NN_Project_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks Project\n",
        "Neural Networks course at Sapienza University of Rome\n",
        "## Breast Cancer Detection: Augmenting Convolutional networks with attention-based aggregation\n",
        "\n",
        "*   Filippo Betello 1835108 \n",
        "*   Federico Carmignani 1845479\n",
        "\n",
        "Reference paper source: https://arxiv.org/pdf/2112.13692.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "khwYNvZdn5lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of Packages\n",
        "Here the Timm package is installed to be used afterwards.\n",
        "PyTorch Image Models (timm) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts."
      ],
      "metadata": {
        "id": "pySFIpIiwST0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeXTamAaxins",
        "outputId": "6ef2bfb5-3641-4f3c-eed9-c7640656d447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n",
            "\u001b[31m[OK] Timm package installed successfully\n"
          ]
        }
      ],
      "source": [
        "#Timm installation\n",
        "print(\"\\u001b[31m\")\n",
        "!pip install timm\n",
        "\n",
        "print(\"\\u001b[31m[OK] Timm package installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6daHG7-_F3pf"
      },
      "source": [
        "## Imports of libraries and dependencies \n",
        "Here the libraries and dependencies used in the implementation are imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcls6aAPxhc0",
        "outputId": "bfbdf9f0-c0eb-4562-c295-4214b83bad3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m[OK] Libraries and dependencies imported successfully\n"
          ]
        }
      ],
      "source": [
        "#System and Data imports\n",
        "import os\n",
        "import json\n",
        "from functools import partial\n",
        "from typing import Optional\n",
        "\n",
        "#MatPlotLib and Numpy imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#TorchVision imports\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets.folder import ImageFolder, default_loader\n",
        "from torchvision.transforms.transforms import Resize\n",
        "\n",
        "#Timm imports\n",
        "import timm\n",
        "from timm.data import create_transform\n",
        "from timm.models.efficientnet_blocks import SqueezeExcite\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "print(\"\\u001b[31m[OK] Libraries and dependencies imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY-tjJ-e1lRq"
      },
      "source": [
        "## Preparation of the dataset: CIFAR10\n",
        "Here the dataset is:\n",
        "\n",
        "1.   downloaded\n",
        "2.   augmented\n",
        "3.   splitted in training and test phases using specific dataloaders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPO0WIX-w_vT"
      },
      "outputs": [],
      "source": [
        "#Building and organizing the dataset (CIFAR10)\n",
        "def build_dataset_CIFAR10():\n",
        "    #Creating Transform element for Augmentation of the dataset in Training phase\n",
        "    transform_train = transforms.Compose([\n",
        "                                          transforms.RandomHorizontalFlip(), #Flips the image w.r.t horizontal axis\n",
        "                                          transforms.RandomRotation(10),     #Rotates the image to a specified angle\n",
        "                                          transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles\n",
        "                                          transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Customizes the color params \n",
        "                                          #transforms.Resize((224, 224)),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "                                         )    \n",
        "    \n",
        "    #Creating Transform element for Resize of the dataset in Testing phase\n",
        "    transform = transforms.Compose([\n",
        "                                    #transforms.Resize((224, 224)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "                                  )\n",
        "    \n",
        "    #Defining the batch size for training and testing phases: as usual a multiple of 2 is chosen\n",
        "    batch_size = 20\n",
        "    \n",
        "    print(\"\\u001b[31m\")\n",
        "\n",
        "    #Training phase: downloading the dataset and creation of the dataloader\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              shuffle=True)\n",
        "    #Test phase: downloading the dataset and creation of the dataloader\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                            shuffle=False)\n",
        "\n",
        "    #Returning dataset chunks and loaders for both Training and Testing phase, as well as the batch size defined\n",
        "    return trainset, trainloader, testset, testloader, batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Enumerating the 10 classes of the dataset CIFAR10 \n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "#Building and splitting the dataset using the method above\n",
        "trainset, trainloader, testset, testloader, batch_size = build_dataset_CIFAR10()\n",
        "\n",
        "print(\"\\u001b[31m[OK] Dataset downloaded and splitted successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "c100a11bf02e4be48b25530fa4c89216",
            "df876ca900574a25a516070ceaa97d95",
            "96d455d1f7774c1aba0d861d7f553d1d",
            "1903a3d7250947069a1961ac20fde860",
            "455fb679570b4336bf94f7915bf30eff",
            "890fe1102a984fc78f8f7f6dbbd90a8f",
            "93c6f79a6b0640079b96e7382dcbcfbd",
            "a1b80bbabe50448382c62d9b61815cf3",
            "c20c8cf7b52a4031aaa9ac3eee901742",
            "f85fcebfe9f2450699ced0013e8634b1",
            "f841ba098d9646059457950e458152d3"
          ]
        },
        "id": "v4nS7fFh33KF",
        "outputId": "9d992ebc-8da6-4ddd-e900-1b1bc5c17a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c100a11bf02e4be48b25530fa4c89216",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "\u001b[31m[OK] Dataset downloaded and splitted successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device configuration\n",
        "A GPU is usually used to exploit its velocity in image processing, if it is not available in Colab then a CPU is assigned for availability reasons."
      ],
      "metadata": {
        "id": "rKjJLr76sldY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ30tg3ELdSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534c0ae6-2b86-46e9-c9f1-a26e62f292be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m cuda:0\n",
            "\u001b[31m[OK] Device configured successfully\n"
          ]
        }
      ],
      "source": [
        "#Setting GPU (Cuda) as default device, otherwise CPU is assigned\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"\\u001b[31m\",device)\n",
        "print(\"\\u001b[31m[OK] Device configured successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysn1bCp9GDPP"
      },
      "source": [
        "## Resnet50 model\n",
        "A residual neural network (ResNet) is an artificial neural network (ANN). Residual neural networks utilize skip connections, or shortcuts to jump over some layers. Typical ResNet models are implemented with double- or triple- layer skips that contain nonlinearities (ReLU) and batch normalization in between.\n",
        "ResNet-50 is a convolutional neural network that is 50 layers deep. The pretrained network can classify images into 1000 object categories. As a result, the network has learned rich feature representations for a wide range of images. The network has an image input size of 224-by-224.\n",
        "We are going to use it as a basis for comparison with the innovative architecture PatchConvNet implemented in the following section. Note that this ResNet50 has a value of output features equal to 10 according to CIFAR10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm2GsOsN7XO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4968eb15-60e7-403a-c70f-3b0c911e0d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m[OK] Resnet50 configured successfully\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=2048, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "\n",
        "\n",
        "#Resnet50 model is saved from PyTorch into the device used\n",
        "model_resnet50 = ResNet50()\n",
        "model = model_resnet50.to(device)\n",
        "\n",
        "#Setting Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#Setting Optimizer as SGD with learning rate 0.01 and weight decay 0.01 to avoid overfitting\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n",
        "\n",
        "print(\"\\u001b[31m[OK] Resnet50 configured successfully\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL PATCH CONV"
      ],
      "metadata": {
        "id": "iLE2MZ99cdGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        hidden_features: Optional[int] = None,\n",
        "        out_features: Optional[int] = None,\n",
        "        act_layer: nn.Module = nn.GELU,\n",
        "        drop: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Learned_Aggregation_Layer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int = 1,\n",
        "        qkv_bias: bool = False,\n",
        "        qk_scale: Optional[float] = None,\n",
        "        attn_drop: float = 0.0,\n",
        "        proj_drop: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim: int = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim**-0.5\n",
        "\n",
        "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.id = nn.Identity()\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, N, C = x.shape\n",
        "        q = self.q(x[:, 0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        q = q * self.scale\n",
        "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = q @ k.transpose(-2, -1)\n",
        "        attn = self.id(attn)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x_cls = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n",
        "        x_cls = self.proj(x_cls)\n",
        "        x_cls = self.proj_drop(x_cls)\n",
        "\n",
        "        return x_cls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Layer_scale_init_Block_only_token(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = False,\n",
        "        qk_scale: Optional[float] = None,\n",
        "        drop: float = 0.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        drop_path: float = 0.0,\n",
        "        act_layer: nn.Module = nn.GELU,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        Attention_block=Learned_Aggregation_Layer,\n",
        "        Mlp_block=Mlp,\n",
        "        init_values: float = 1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention_block(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop\n",
        "        )\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
        "        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, x_cls: torch.Tensor) -> torch.Tensor:\n",
        "        u = torch.cat((x_cls, x), dim=1)\n",
        "        x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))\n",
        "        x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))\n",
        "        return x_cls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Conv_blocks_se(nn.Module):                                # UNO column/trunk va moltiplicato xN = depth\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.qkv_pos = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(dim, dim, groups=dim, kernel_size=3, padding=1, stride=1, bias=True),\n",
        "            nn.GELU(),\n",
        "            SqueezeExcite(dim, rd_ratio=0.25),\n",
        "            nn.Conv2d(dim, dim, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, N, C = x.shape\n",
        "        H = W = int(N ** 0.5)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = x.reshape(B, C, H, W)\n",
        "        x = self.qkv_pos(x)\n",
        "        x = x.reshape(B, C, N)\n",
        "        x = x.transpose(-1, -2)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Layer_scale_init_Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        drop_path: float = 0.0,\n",
        "        act_layer: nn.Module = nn.GELU,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        Attention_block=None,\n",
        "        init_values: float = 1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention_block(dim)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> nn.Sequential:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "class ConvStem(nn.Module):\n",
        "    \"\"\"Image to Patch Embedding\"\"\"\n",
        "\n",
        "    def __init__(self, img_size: int = 32, patch_size: int = 4, in_chans: int = 3, embed_dim: int = 768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)          # (32, 32)\n",
        "        patch_size = to_2tuple(patch_size)      # (4, 4)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])           # 64\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Sequential(                          # da 3 a embed_dim dimensione dei planes        # maybe change GELU with RELU ??\n",
        "            conv3x3(in_chans, embed_dim // 8, 2),\n",
        "            nn.GELU(),\n",
        "            conv3x3(embed_dim // 8, embed_dim // 4, 2),\n",
        "            nn.GELU(),\n",
        "            conv3x3(embed_dim // 4, embed_dim // 2, 2),\n",
        "            nn.GELU(),\n",
        "            conv3x3(embed_dim // 2, embed_dim, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, padding_size: Optional[int] = None) -> torch.Tensor:\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchConvnet(nn.Module):\n",
        "\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10, embed_dim=768, depth=12,\n",
        "                 num_heads=1, qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, global_pool=None,\n",
        "                 block_layers = Layer_scale_init_Block,\n",
        "                 block_layers_token = Layer_scale_init_Block_only_token,\n",
        "                 Patch_layer=ConvStem,act_layer=nn.GELU,\n",
        "                 Attention_block = Conv_blocks_se ,\n",
        "                dpr_constant=True,init_scale=1e-4,\n",
        "                Attention_block_token_only=Learned_Aggregation_Layer,\n",
        "                Mlp_block_token_only= Mlp,\n",
        "                depth_token_only=1,\n",
        "                mlp_ratio_clstk = 3.0,\n",
        "                multiclass=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multiclass = multiclass\n",
        "        self.patch_size = patch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "\n",
        "        self.patch_embed = Patch_layer(     # convolutional steam\n",
        "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        \n",
        "        if not self.multiclass:             # monoclass --> crea un solo token per tutte le classi matrice di 0 di 768dim\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, int(embed_dim)))\n",
        "        \n",
        "        dpr = [drop_path_rate for i in range(depth)]        # ARRAY DI 0 DI LEN = DEPTH = 12\n",
        "            \n",
        "        self.blocks = nn.ModuleList([                       # SINGOLI BLOCCHI DENTRO COLUMN TRUNCK DI NUM = DEPTH\n",
        "            block_layers(\n",
        "                dim=embed_dim, drop_path=dpr[i], norm_layer=norm_layer,\n",
        "                act_layer=act_layer,Attention_block=Attention_block,init_values=init_scale)\n",
        "            for i in range(depth)])\n",
        "                    \n",
        "        \n",
        "        self.blocks_token_only = nn.ModuleList([            # SINGOLI BLOCCHI DENTRO COLUMN TRUNCK DI NUM = DEPTH + TOKEN\n",
        "            block_layers_token(\n",
        "                dim=int(embed_dim), num_heads=num_heads, mlp_ratio=mlp_ratio_clstk,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0.0, norm_layer=norm_layer,\n",
        "                act_layer=act_layer,Attention_block=Attention_block_token_only,\n",
        "                Mlp_block=Mlp_block_token_only,init_values=init_scale)\n",
        "            for i in range(depth_token_only)])\n",
        "        \n",
        "        self.norm = norm_layer(int(embed_dim))              # NORMALIZATION PART\n",
        "        \n",
        "        self.total_len = depth_token_only+depth             # PROFONDIT TOTALE DELLA RETE --> 1+12 = 13\n",
        "        \n",
        "        self.feature_info = [dict(num_chs=int(embed_dim ), reduction=0, module='head')]         # [{'num_chs': 768, 'reduction': 0, 'module': 'head'}]  DICTIONARY\n",
        "        if not self.multiclass:\n",
        "            self.head = nn.Linear(int(embed_dim), num_classes) if num_classes > 0 else nn.Identity()        # PAPER DA INFO SUL PERCHè USIAMO QUESTO --> IN PRIMA PAGINA\n",
        "        \n",
        "\n",
        "\n",
        "        trunc_normal_(self.cls_token, std=.02)              # FUNCTION IMPORTED FROM TIMM.LAYERS\n",
        "        self.apply(self._init_weights)\n",
        "        ################################################ FINE INIT  #########################################################\n",
        "\n",
        "    def _init_weights(self, m):         ## DA CAPIRE\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token'}\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "    \n",
        "    def get_num_layers(self):\n",
        "        return len(self.blocks)\n",
        "    \n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        \n",
        "        for i , blk in enumerate(self.blocks):\n",
        "            x  = blk(x)\n",
        "\n",
        "        for i , blk in enumerate(self.blocks_token_only):\n",
        "            cls_tokens = blk(x,cls_tokens)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        \n",
        "        if not self.multiclass:\n",
        "            return x[:, 0]\n",
        "        else:\n",
        "            return x[:, :self.num_classes].reshape(B,self.num_classes,-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x  = self.forward_features(x)\n",
        "        if not self.multiclass:\n",
        "            x = self.head(x)\n",
        "            return x\n",
        "        else:\n",
        "            all_results = []\n",
        "            for i in range(self.num_classes):\n",
        "                all_results.append(self.head[i](x[:,i]))\n",
        "            return torch.cat(all_results,dim=1).reshape(B,self.num_classes)\n",
        "\n",
        "@register_model\n",
        "def S60(pretrained: bool = False, **kwargs):\n",
        "    model = PatchConvnet(\n",
        "        patch_size=4,\n",
        "        embed_dim=384,\n",
        "        depth=60,\n",
        "        num_heads=1,\n",
        "        qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "        Patch_layer=ConvStem,\n",
        "        Attention_block=Conv_blocks_se,\n",
        "        depth_token_only=1,\n",
        "        mlp_ratio_clstk=3.0,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return model\n",
        "        "
      ],
      "metadata": {
        "id": "RhH_LRbEcgns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = S60()\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "EZ5-Otl_cv3J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "3e60e37c-cb4c-4abe-8cd7-77b1c5efb8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-e1526eab478e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS60\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)   #ADAM"
      ],
      "metadata": {
        "id": "iCyEE4mSdDtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lamb optimizer"
      ],
      "metadata": {
        "id": "geqnO5b4jpZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeZiN6DejreO",
        "outputId": "d973796c-7007-4f8c-adaf-f5442bfaab56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 474 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.10.0+cu111)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_optimizer as optime\n",
        "\n",
        "optimizer = optime.Lamb(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "Wvo7ujTLjuaZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "77221dd4-d3d1-44b2-c0d2-3ddc39e7f2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9acfac721437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_optimizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLamb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0aKF5e4GHpK"
      },
      "source": [
        "## Mounting drive folders\n",
        "This job is done to save models pretrained."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # import drive from google colab\n",
        "import time\n",
        "print(\"\\u001b[31m\")\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/NN/Model/patch_0101.pth'\n",
        "\n",
        "print(\"\\u001b[31m[OK] Drive mounted successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8gmYjji4AOk",
        "outputId": "99b66bb9-ea22-4f21-e4e6-119f7cb1d4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[31m[OK] Drive mounted successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EarlyStopping\n",
        "Early stopping is a form of regularization used to avoid overfitting on the training dataset. Early stopping keeps track of the validation loss, if the loss stops decreasing for several epochs in a row the training stops. The EarlyStopping class in is used to create an object to keep track of the validation loss while training a PyTorch model. It will save a checkpoint of the model each time the validation loss decrease. We set the patience argument in the EarlyStopping class to how many epochs we want to wait after the last time the validation loss improved before breaking the training loop.\n",
        "This because when training a pretrained one, overfitting is possible: training accuracy is greater than validation one."
      ],
      "metadata": {
        "id": "eqqdlI18U7-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining EarlyStopping to avoid overfitting during training\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "print(\"\\u001b[31m[OK] EarlyStopping defined successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFJ4c5pdU7gr",
        "outputId": "3aca4313-3ecd-4bf0-fe30-7b0e9deb2a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m[OK] EarlyStopping defined successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation"
      ],
      "metadata": {
        "id": "rryIrETOWlM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "running_loss_history = []\n",
        "running_corrects_history = []\n",
        "val_running_loss_history = []\n",
        "val_running_corrects_history = []\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "## Transfer learning with fine tuning\n",
        "#model.load_state_dict(torch.load(PATH))\n",
        "#model.eval()\n",
        "\n",
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0.0\n",
        "    val_running_loss = 0.0\n",
        "    val_running_corrects = 0.0\n",
        "    total = 0\n",
        "    total_val = 0\n",
        " \n",
        "    model.train() #prepare for training\n",
        "    for inputs, labels in trainloader:\n",
        "\n",
        "        #print(inputs)\n",
        "        \n",
        "        inputs = inputs.to(device, non_blocking=True) # input to device as our model is running in mentioned device.\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        outputs = model(inputs) #the output will be a tensor element of size 10 (num_classes), then the max is taken to address the predcited class\n",
        "       # print(outputs.shape)\n",
        "        # zero the parameter gradients\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1) #the max is taken to address the predcited class\n",
        "        total += labels.size(0)\n",
        "        running_loss += loss.item() \n",
        "        running_corrects += (preds == labels.data).sum().item()\n",
        "\n",
        "    else:\n",
        "\n",
        "      with torch.no_grad(): # we do not need gradient for validation.\n",
        "        model.eval() # prep model for evaluation\n",
        "        for val_inputs, val_labels in testloader:\n",
        "          val_inputs = val_inputs.to(device)\n",
        "          val_labels = val_labels.to(device)\n",
        "          val_outputs = model(val_inputs)\n",
        "          val_loss = criterion(val_outputs, val_labels)\n",
        "          \n",
        "          _, val_preds = torch.max(val_outputs, 1)\n",
        "          total_val +=  val_labels.size(0)\n",
        "          val_running_loss += val_loss.item()\n",
        "          val_running_corrects += (val_preds == val_labels.data).sum().item()\n",
        "      \n",
        "    epoch_loss = running_loss/len(trainloader) # loss per epoch\n",
        "    epoch_acc = 100 * running_corrects/total # accuracy per epoch\n",
        "    running_loss_history.append(epoch_loss) # appending for displaying \n",
        "    running_corrects_history.append(epoch_acc)\n",
        "    \n",
        "    val_epoch_loss = val_running_loss/len(testloader)\n",
        "    val_epoch_acc = 100 * val_running_corrects/total_val\n",
        "    val_running_loss_history.append(val_epoch_loss)\n",
        "    val_running_corrects_history.append(val_epoch_acc)\n",
        "    print('epoch :', (epoch+1))\n",
        "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc))\n",
        "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc))\n",
        "    \n",
        "     #early_stopping needs the validation loss to check if it has decresed, \n",
        "        #and if it has, it will make a checkpoint of the current model\n",
        "    early_stopping(val_epoch_loss, model)\n",
        "        \n",
        "    if early_stopping.early_stop:\n",
        "          print(\"Early stopping\")\n",
        "          break\n",
        "\n",
        "#load the last checkpoint with the best model\n",
        "model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "HgLB6NpoU21F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "5190371c-93fe-4135-b7f3-b021517ca69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-5560f4982c6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input to device as our model is running in mentioned device.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#the output will be a tensor element of size 10 (num_classes), then the max is taken to address the predcited class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m        \u001b[0;31m# print(outputs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bbc172d8437f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mx\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bbc172d8437f>\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks_token_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mcls_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcls_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bbc172d8437f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, x_cls)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mx_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_cls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mx_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_cls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bbc172d8437f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Results"
      ],
      "metadata": {
        "id": "cMfoX-cnt6vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(running_loss_history, label='training loss')\n",
        "plt.plot(val_running_loss_history, label='validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train vs Val Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "mx2uz3dzq9SI",
        "outputId": "bf237325-a6a5-4af3-8173-3d1e3cab995c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5a85dcded0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(running_corrects_history, label='training accuracy')\n",
        "plt.plot(val_running_corrects_history, label='validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train vs Val Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "zcG35YlMrEWb",
        "outputId": "63843ff1-a8cd-40e6-f45a-39e274223299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5a85f71410>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print output"
      ],
      "metadata": {
        "id": "jMixY4SYrq1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0xL4nLnD1xm",
        "outputId": "436bf21b-36cf-45a1-937e-858a79d700b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit-pytorch in /usr/local/lib/python3.7/dist-packages (0.26.7)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (0.11.1+cu111)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->vit-pytorch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->vit-pytorch) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->vit-pytorch) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import warnings\n",
        "\n",
        "from vit_pytorch.recorder import Recorder, find_modules\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "0hrplybQr1Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class visualizer(nn.Module):\n",
        "    #Slightly modified code from Lucidrains (https://github.com/lucidrains)\n",
        "    #Original code: https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/recorder.py\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "        self.data = None\n",
        "        self.recordings = []\n",
        "        self.hooks = []\n",
        "        self.hook_registered = False\n",
        "        self.ejected = False\n",
        "\n",
        "    def _hook(self, _, input, output):\n",
        "        self.recordings.append(output.clone().detach())\n",
        "\n",
        "        \n",
        "    def _register_hook(self):\n",
        "        handle = self.model.blocks_token_only[0].attn.id.register_forward_hook(self._hook)\n",
        "        self.hooks.append(handle)\n",
        "        self.hook_registered = True\n",
        "        \n",
        "    def eject(self):\n",
        "        self.ejected = True\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks.clear()\n",
        "        return self.model\n",
        "\n",
        "    def clear(self):\n",
        "        self.recordings.clear()\n",
        "\n",
        "    def record(self, x):\n",
        "        recording = x.clone().detach()\n",
        "        self.recordings.append(recording)\n",
        "\n",
        "    def forward(self, img):\n",
        "        assert not self.ejected, 'recorder has been ejected, cannot be used anymore'\n",
        "        self.clear()\n",
        "\n",
        "        if not self.hook_registered:\n",
        "            self._register_hook()\n",
        "\n",
        "        pred = self.model(img)\n",
        "        return pred, self.recordings  "
      ],
      "metadata": {
        "id": "_piRqVv-r7Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "def get_transforms_original(input_size=32,test_size=32,mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]):\n",
        "\n",
        "    transformations = transforms.Compose(\n",
        "        [transforms.Resize(int(test_size), interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
        "         transforms.CenterCrop(test_size)])\n",
        "    \n",
        "    return transformations\n",
        "\n",
        "def get_transforms(input_size=32, test_size=32,mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]):\n",
        "\n",
        "    transformations = transforms.Compose(\n",
        "        [transforms.Resize(int(test_size), interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
        "         transforms.CenterCrop(test_size),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean, std)])\n",
        "\n",
        "    return transformations"
      ],
      "metadata": {
        "id": "cspVZPD2r7vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://huggingface.co/proxy-datasets-preview/assets/cifar10/--/plain_text/train/0/img/image.jpg'\n",
        "#url = 'https://huggingface.co/proxy-datasets-preview/assets/cifar10/--/plain_text/train/3/img/image.jpg'\n",
        "#url = 'https://huggingface.co/proxy-datasets-preview/assets/cifar10/--/plain_text/train/9/img/image.jpg'\n",
        "#url = 'https://huggingface.co/proxy-datasets-preview/assets/cifar10/--/plain_text/train/15/img/image.jpg'\n",
        "im = Image.open(requests.get(url, stream=True).raw)\n",
        "image_size = 32"
      ],
      "metadata": {
        "id": "ljCHWhcoviqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "3560a2a3-cef2-47f2-cd9f-3bff968a838e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f06f11519549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#url = 'https://huggingface.co/proxy-datasets-preview/assets/cifar10/--/plain_text/train/9/img/image.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#url = 'https://huggingface.co/proxy-datasets-preview/assets/cifar10/--/plain_text/train/15/img/image.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = S60()\n",
        "PATH = '/content/drive/MyDrive/NN/Model/patch_0101.pth'\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model = model.eval()\n",
        "#remove gradients\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "# add visualizer to get the attention map\n",
        "model = visualizer(model)"
      ],
      "metadata": {
        "id": "mJ-9TWWFyojA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "#generate org image\n",
        "data_aug_org =get_transforms_original(im.size[0],im.size[1])\n",
        "im = data_aug_org(im)\n",
        "#generate sample image\n",
        "data_aug = get_transforms(image_size,image_size)\n",
        "sample = data_aug(im)\n",
        "sample = sample.unsqueeze(0)\n",
        "im = data_aug_org(im)\n",
        "#plot original image\n",
        "axs[0].imshow((np.array(1)* im))\n",
        "axs[0].grid(None)\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Original image')\n",
        "#get attention maps\n",
        "print(sample)\n",
        "output,attn  = model(sample) #******da vedere bene perche*****\n",
        "ps = int(image_size/16)\n",
        "#softmax score without class token\n",
        "attn2 = attn[0].clone().reshape(ps*ps+1)[1:].softmax(dim=-1).reshape((ps,ps))\n",
        "mask = attn2.detach().numpy()\n",
        "#resize attention map to original image size\n",
        "mask = cv2.resize(mask/mask.max(), im.size)[..., np.newaxis]\n",
        "#normalize attention map\n",
        "mask =((mask - mask.min())/(mask.max() - mask.min()))\n",
        "result = ((mask) * im).astype(\"uint8\")\n",
        "result = rgb2gray(result)\n",
        "#plot attention maps\n",
        "axs[1].imshow(result,cmap='cividis')\n",
        "axs[1].grid(None)\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('Attention map')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "EGMn-nbdrisF",
        "outputId": "922a024b-7481-4ecf-9164-4e7c28ba02ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.9059, -0.6392, -0.8196,  ..., -0.9059, -0.6549,  0.2941],\n",
            "          [-0.9451, -0.5137, -0.4588,  ..., -0.9373, -0.8118,  0.0196],\n",
            "          [-0.4118, -0.2078, -0.3569,  ..., -0.9608, -0.9294, -0.1765],\n",
            "          ...,\n",
            "          [ 0.5686,  0.5059,  0.6157,  ...,  0.6392,  0.1529, -0.0275],\n",
            "          [ 0.4275,  0.2706,  0.3490,  ...,  0.2000, -0.0588,  0.0039],\n",
            "          [ 0.3333,  0.0588,  0.0510,  ..., -0.0118, -0.0824,  0.2000]],\n",
            "\n",
            "         [[-0.3255, -0.0745, -0.2941,  ..., -0.9608, -0.7098,  0.2392],\n",
            "          [-0.4118,  0.0039,  0.0196,  ..., -0.9608, -0.8510, -0.0196],\n",
            "          [ 0.0588,  0.2314,  0.0431,  ..., -0.9608, -0.9294, -0.1922],\n",
            "          ...,\n",
            "          [ 0.7412,  0.6784,  0.7961,  ...,  0.7333,  0.2392,  0.0353],\n",
            "          [ 0.4588,  0.3176,  0.4039,  ...,  0.2863,  0.0196,  0.0588],\n",
            "          [ 0.2706,  0.0118,  0.0275,  ...,  0.0745, -0.0039,  0.2549]],\n",
            "\n",
            "         [[-0.9294, -0.6549, -0.8353,  ..., -1.0000, -0.7725,  0.1922],\n",
            "          [-1.0000, -0.5608, -0.5059,  ..., -1.0000, -0.8824, -0.0431],\n",
            "          [-0.5216, -0.3255, -0.4745,  ..., -0.9765, -0.9137, -0.1686],\n",
            "          ...,\n",
            "          [ 0.0980,  0.0118,  0.0902,  ...,  0.3569, -0.0902, -0.2549],\n",
            "          [ 0.2471,  0.0510,  0.0745,  ..., -0.0510, -0.2706, -0.2000],\n",
            "          [ 0.3725,  0.0431, -0.0431,  ..., -0.2392, -0.2706,  0.0118]]]])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NN_Project_CIFAR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ysn1bCp9GDPP",
        "eqqdlI18U7-a"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c100a11bf02e4be48b25530fa4c89216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df876ca900574a25a516070ceaa97d95",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96d455d1f7774c1aba0d861d7f553d1d",
              "IPY_MODEL_1903a3d7250947069a1961ac20fde860",
              "IPY_MODEL_455fb679570b4336bf94f7915bf30eff"
            ]
          }
        },
        "df876ca900574a25a516070ceaa97d95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96d455d1f7774c1aba0d861d7f553d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_890fe1102a984fc78f8f7f6dbbd90a8f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93c6f79a6b0640079b96e7382dcbcfbd"
          }
        },
        "1903a3d7250947069a1961ac20fde860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a1b80bbabe50448382c62d9b61815cf3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c20c8cf7b52a4031aaa9ac3eee901742"
          }
        },
        "455fb679570b4336bf94f7915bf30eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f85fcebfe9f2450699ced0013e8634b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 55327351.15it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f841ba098d9646059457950e458152d3"
          }
        },
        "890fe1102a984fc78f8f7f6dbbd90a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93c6f79a6b0640079b96e7382dcbcfbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1b80bbabe50448382c62d9b61815cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c20c8cf7b52a4031aaa9ac3eee901742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f85fcebfe9f2450699ced0013e8634b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f841ba098d9646059457950e458152d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}